# Streaming de dados do Kafka Topic para Spark usando Spark Structured Streaming V2.0

Esta vers√£o √© otimizada para ser mais leve. Tudo √© executado a partir de um simples comando. N√£o h√° nenhuma depend√™ncia em seu computador al√©m do docker e do docker-compose.

## fluxo de trabalho

Este projeto √© uma arquitetura simples de kafka e streaming de spark.
Um arquivo docker-compose inicializa um cluster kafka e um cluster spark com todas as suas depend√™ncias.
Os produtores enviam mensagens de texto para kafka um t√≥pico chamado "test-topic".
Voc√™ pode consumir mensagens com scripts de consumidores escritos em NodeJS e Python ou transmitir dados com streaming de spark, que simplesmente imprime no console todos os dados recebidos.

> Aviso: o streaming de dados s√≥ funciona com spark em scala por enquanto, a vers√£o python est√° em andamento

<img src="architecture.png" />

Tudo √© autom√°tico neste projeto.

Tudo o que voc√™ precisa fazer √© executar um script simples que acionar√° tudo.

Voc√™ pode mergulhar mais fundo no c√≥digo e brincar com ele para sujar as m√£os üòä

## Requisitos

- Docker e Docker Compose (https://docs.docker.com/engine/install/ubuntu/)
  - janela de encaixe >= 19.X.X
  - docker-compose ~1.29.2

> Certifique-se de que:

- voc√™ pode executar comandos com privil√©gios de root em seu computador
- sua porta 8080 n√£o est√° em uso
- a sub-rede 172.18.0.0/24 n√£o est√° em uso em seu computador

## Estrutura de pastas do projeto

```
.
‚îú‚îÄ‚îÄ architecture.png........ # Arquitetura do projeto
‚îú‚îÄ‚îÄ clean-env.sh............ # Limpa o ambiente
‚îú‚îÄ‚îÄ docker-compose.yml...... # Criar kafka e acender clusters
‚îú‚îÄ‚îÄ nodejs-consumer......... # Consome mensagens de kafka
‚îÇ ‚îú‚îÄ‚îÄ consumidor.js
‚îÇ ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ ‚îú‚îÄ‚îÄ pacote.json
‚îÇ ‚îî‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ nodejs-producer......... # Produz mensagens para kafka
‚îÇ ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ ‚îú‚îÄ‚îÄ pacote.json
‚îÇ ‚îî‚îÄ‚îÄ produtor.js
‚îú‚îÄ‚îÄ python-consumer......... # Consome mensagens para kafka
‚îÇ ‚îú‚îÄ‚îÄ consumidor.py
‚îÇ ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ python-producer......... # Produz mensagens para kafka
‚îÇ ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ ‚îî‚îÄ‚îÄ produtor.py
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ spark-streaming......... # Consumir dados de streaming do kafka e afundar no console
      ‚îú‚îÄ‚îÄ python.............. # Streaming com python (Work In Progress)
      ‚îî‚îÄ‚îÄ scala............... # Streaming com scala
```

## Executando servi√ßos

| nome do servi√ßo              | endere√ßo[:porta] |
| ---------------------------- | ---------------- |
| tratador                     | 172.18.0.8:2181  |
| kafka (do host)              | 172.18.0.9:9093  |
| kafka (dentro do recipiente) | 172.18.0.9:9092  |
| spark mestre                 | 172.18.0.10:7077 |
| spark interface do usu√°rio   | 172.18.0.10:8080 |
| spark trabalhador 1          | 172.18.0.11      |
| spark trabalhador 2          | 172.18.0.12      |
| spark-streaming-kafka        | 172.18.0.13      |
| nodejs-produtor              | 172.18.0.14      |
| nodejs-consumer              | 172.18.0.15      |
| produtor de python           | 172.18.0.16      |
| python-consumidor            | 172.18.0.17      |

O projeto cria um nome de rede docker "kafka-spark" no intervalo de endere√ßos 172.18.0.0/24

## Come√ßando

> Observa√ß√£o: voc√™ pode acessar os arquivos docker-compose.yml ou run.sh para entender melhor como as coisas funcionam.

### 1. Clone o reposit√≥rio e o cd na pasta

> Nota: Certifique-se de estar no diret√≥rio <kafka-spark-streaming-docker>

```
      git clone https://github.com/MDiakhate12/kafka-spark-streaming-docker.git
      cd kafka-spark-streaming-docker/
```

### 2. Execute docker-compose.yml

> Importante: N√£o feche o terminal depois de executar o docker-compose <br>

```
docker-compose
```

> Nota: Aguarde at√© que todos os servi√ßos estejam ativos (cerca de 1 a 2 minutos, o console ficar√° bastante ocioso)

### 3. Envie o trabalho de streaming do Spark

> Nota: Certifique-se de ter privil√©gios de root

Em um novo terminal, execute o comando

```bash
sudo chmod 777 jars_dir && \
docker exec -it spark \
enviar spark \
--packages "org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0" \
--master "spark://172.18.0.10:7077" \
--class Transmiss√£o \
--conf spark.jars.ivy=/opt/bitnami/spark/ivy \
ivy/spark-streaming-with-kafka_2.12-1.0.jar
```

Depois que tudo estiver definido, sua sa√≠da deve ficar assim:

![Captura de tela de 15/11/2021 15/05/41](https://user-images.githubusercontent.com/46793415/141721499-a248453e-4a7f-4d5e-88ea-c353de7922b9.png)

√â isso üéâüéâ Parab√©ns.

## Olha o resultado

> Observa√ß√£o: a IU do Spark est√° dispon√≠vel em http://172.18.0.10:8080

Em um novo terminal, voc√™ pode ver os logs de cada servi√ßo executando:

```
docker-compose logs -f [SERVICE_NAME]
```

Os servi√ßos dispon√≠veis s√£o:

1. tratador
2. kafka
3. spark
4. spark-worker-1
5. trabalhador-spark-2
6. spark-streaming-kafka
7. nodejs-produtor
8. nodejs-consumer
9. produtor de python
10. consumidor de python
